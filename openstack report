
Network Name and ID:
- Name: c30-oa-prod-oa-80-network
- Network ID: ec40bccd-c7d2-4dcb-9cbd-2f0369b7dc8f
2. Subnets Attached to OA80:
- ComputeSubnet2a: dc0bcb6d-c916-4b25-aaec-f5b76a6dd7ab
- ComputeSubnet2b: [Example ID, not shown]
- ComputeSubnet2c: [Example ID, not shown]
- StorageSubnet: [Example ID, not shown]
3. Router Details:
- Router Name: c30-oa-prod-oa-80-RtbInternal-rt
- Router External Gateway Subnet ID: 08912802-1f16-471f-8bd1-385b36d94651
- Connected to external VLAN: pro-net-vlan3160
4. Port Creation in Script:
- Command: openstack port create --network ec40bccd... --fixed-ip subnet=dc0bcb6d... --enable
oa80_big_mem_test
- This creates a port in ComputeSubnet2a
5. Floating IP Creation in Script:
- Command uses:
--subnet 08912802... (matches VLAN 3160)
--port $port_id (created above)
6. Volume Created:
- From snapshot ID: 36748f2c-...
- Volume size: 320 GB, bootable
7. Server Creation:
- Flavor: m6.large-mem
- Uses created volume, port, and security groups
8. Mappings Recap:
- Network: ec40bccd...
- Subnet for Port: dc0bcb6d... (2a)
- External Subnet for Floating IP: 08912802... --> VLAN 3160
9. Validation Commands:
- openstack network list | grep oa-80
- openstack subnet list --network ec40bccd...
- openstack router show <router_id> to see external subnet
- openstack network list --external to get VLAN from subnet
This report outlines how OA80's internal and external components map to create floating IPs, ports, and VMs
within a production OpenStack setup






#!/bin/bash

echo "=== OpenStack Instance Creation Script ==="

# Prompt for instance name
read -p "Enter instance name: " INSTANCE_NAME

# Prompt for image (OS)
echo "Available images:"
openstack image list --column Name --format value
read -p "Enter image name (OS): " IMAGE_NAME

# Prompt for flavor (T-shirt size)
echo "Available flavors (T-shirt sizes):"
openstack flavor list --column Name --format value
read -p "Enter flavor (e.g., m1.small): " FLAVOR_NAME

# Prompt for network
echo "Available networks:"
openstack network list --column Name --format value
read -p "Enter network name: " NETWORK_NAME

# Prompt for key pair
echo "Available key pairs:"
openstack keypair list --column Name --format value
read -p "Enter key pair name: " KEY_NAME

# Prompt for security group(s)
echo "Available security groups:"
openstack security group list --column Name --format value
read -p "Enter security group(s) (comma-separated if multiple): " SECURITY_GROUPS

# Convert comma-separated SGs to --security-group arguments
IFS=',' read -ra SG_ARRAY <<< "$SECURITY_GROUPS"
SG_ARGS=""
for sg in "${SG_ARRAY[@]}"; do
    SG_ARGS+="--security-group $sg "
done

# Confirm before creation
echo
echo "About to create instance:"
echo "Name: $INSTANCE_NAME"
echo "Image: $IMAGE_NAME"
echo "Flavor: $FLAVOR_NAME"
echo "Network: $NETWORK_NAME"
echo "Key Pair: $KEY_NAME"
echo "Security Groups: $SECURITY_GROUPS"
echo
read -p "Proceed? (y/n): " CONFIRM

if [[ "$CONFIRM" != "y" ]]; then
    echo "Aborted."
    exit 1
fi

# Create the instance
openstack server create \
    --image "$IMAGE_NAME" \
    --flavor "$FLAVOR_NAME" \
    --network "$NETWORK_NAME" \
    --key-name "$KEY_NAME" \
    $SG_ARGS \
    "$INSTANCE_NAME"




openstack flavor list -f value -c Name | while read flavor; do
  props=$(openstack flavor show "$flavor" -f value -c properties)
  if [[ -z "$props" || "$props" == "{}" ]]; then
    echo "$flavor ❌ has NO extra_specs"
  else
    echo "$flavor ✅ has extra_specs: $props"
  fi
done



openstack flavor list -f value -c Name | while read flavor; do
  props=$(openstack flavor show "$flavor" -f value -c properties)
  if [[ -z "$props" ]]; then
    echo "$flavor has NO extra specs ❌"
  fi
done



Title: Provisioning Large-Memory VMs on Large-Memory Servers in OpenStack

Objective:
Ensure that virtual machines requiring large amounts of memory are only scheduled on physical compute hosts that have sufficient memory capacity. This is achieved through host aggregates, flavor metadata, scheduler filters, and optional affinity policies.

⸻

Process Overview:
	1.	Identify and group large-memory compute nodes.
	2.	Tag these compute nodes using host aggregates and metadata.
	3.	Create a VM flavor representing a large-memory workload.
	4.	Attach metadata to the flavor that matches the host aggregate.
	5.	Enable and use the appropriate scheduler filters.
	6.	Launch the VM with the large-memory flavor.
	7.	Optionally, apply affinity or anti-affinity rules.

⸻

Detailed Steps:

Step 1: Group large-memory compute nodes
	•	Determine which physical compute nodes in the infrastructure have high memory capacity (e.g., 256 GB or more).
	•	These nodes are to be logically grouped for scheduling control.

Step 2: Create a host aggregate
	•	Use OpenStack to define a host aggregate.
	•	This is a logical grouping that allows you to tag selected hosts with specific metadata (e.g., large_mem=true).
	•	Add the high-memory compute nodes to this aggregate.

Step 3: Assign metadata to the aggregate
	•	Attach key-value metadata to the aggregate that identifies it as intended for large-memory workloads.
	•	Example metadata: large_mem=true

Step 4: Create a flavor for large-memory VMs
	•	Define a flavor with high RAM, high vCPU, and appropriate disk size for large-memory VM use cases.
	•	Assign extra_specs to the flavor that match the host aggregate’s metadata.
	•	This ensures the scheduler only selects hosts within the correct aggregate.

Step 5: Ensure required scheduler filters are enabled
	•	The filter AggregateInstanceExtraSpecsFilter must be enabled in the Nova scheduler configuration.
	•	This filter allows the scheduler to match flavor extra_specs with host aggregate metadata.
	•	If the filter is not active, metadata-based placement will not work.

Step 6: Launch the VM using the custom flavor
	•	When the user launches a VM using the large-memory flavor, the Nova scheduler filters hosts.
	•	Only those hosts that match the aggregate metadata and have sufficient free resources (RAM, CPU) are considered.
	•	The scheduler selects the best-fit host and boots the VM.

Step 7 (Optional): Apply affinity or anti-affinity policies
	•	Affinity rules ensure that VMs are placed on the same host (useful for low-latency communication between VMs).
	•	Anti-affinity rules ensure VMs are spread across different hosts (useful for high availability).
	•	These rules are implemented using server groups and scheduler hints.
	•	If using anti-affinity for large-memory VMs, ensure multiple large-memory hosts exist, or the scheduler will fail to place the second VM.

⸻

Behavior Scenarios:
	1.	Matching host available:
The VM is successfully scheduled to one of the tagged large-memory compute nodes with sufficient available resources.
	2.	No matching host found:
The request fails with an error like NoValidHost. Causes may include:

	•	No compute host with large_mem=true has enough resources.
	•	Flavor metadata does not match any host aggregate.
	•	Scheduler filter is not enabled.

	3.	Filters not properly configured:
If the AggregateInstanceExtraSpecsFilter is disabled, the scheduler may ignore the flavor’s extra_specs and place the VM on any available compute node, potentially violating isolation requirements.

⸻

Responsibility Matrix:
	•	OpenStack Admin: Create aggregates, configure metadata, define flavors, and enable filters.
	•	Infrastructure Team: Identify and manage large-memory servers.
	•	Cloud Users: Use the correct flavor when launching high-memory VMs and apply affinity rules as needed.

⸻

Considerations:
	•	Ensure consistent metadata between flavors and host aggregates.
	•	Avoid overprovisioning large-memory hosts since their capacity is limited.
	•	Use anti-affinity if high availability is needed across different hosts.
	•	Monitor available resources using OpenStack CLI or dashboard to avoid scheduling failures.

⸻

This documentation provides a structured approach to enforcing large-memory workload placement in OpenStack environments, while also supporting placement control through affinity policies.