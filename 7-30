
rm -rf /root/nvidia/python39-offline
rm -f /root/nvidia/python39-offline.tar.gz
ðŸ’» 1. On Your Ubuntu/Linux Machine (with internet)
Build Python 3.9
bash
Copy
Edit
cd /tmp
wget https://www.python.org/ftp/python/3.9.18/Python-3.9.18.tgz
tar -xzf Python-3.9.18.tgz
cd Python-3.9.18
./configure --enable-optimizations
make -j$(nproc)
make install DESTDIR=$HOME/python39-offline
ðŸ“¦ 2. Package Python
bash
Copy
Edit
cd ~
tar -czvf python39-offline.tar.gz python39-offline/
âœ… File size should be ~120 MB.

ðŸ“¤ 3. Copy Python to RHEL VM
bash
Copy
Edit
scp -i veera-dev2.pem ~/python39-offline.tar.gz root@<VM-IP>:/root/nvidia/
ðŸ“‚ 4. On VM: Extract and test
bash
Copy
Edit
cd /root/nvidia
tar -xzvf python39-offline.tar.gz
/root/nvidia/python39-offline/usr/local/bin/python3.9 --version
âœ… Expected:

nginx
Copy
Edit
Python 3.9.18
ðŸ§ª 5. Create & activate virtual environment
bash
Copy
Edit
/root/nvidia/python39-offline/usr/local/bin/python3.9 -m venv /root/torch-env
source /root/torch-env/bin/activate
ðŸ“¦ 6. Install PyTorch from offline .whl
Assuming your PyTorch wheels are already in:

swift
Copy
Edit
/root/nvidia/pytorch-cu121-offline/
Run:

bash
Copy
Edit
pip install --no-index --find-links=/root/nvidia/pytorch-cu121-offline torch torchvision torchaudio
ðŸš€ 7. Test GPU access with PyTorch
bash
Copy
Edit
python -c "import torch; print(torch.__version__); print(torch.cuda.is_available()); print(torch.cuda.get_device_name(0))"
âœ… Expected Output:

graphql
Copy
Edit
2.x.x
True
NVIDIA H100 SXM
Let me know if you want a PyTorch test script for GPU benchmarking or TensorFlow added ne















âœ… STEP-BY-STEP (on your VM)
1. Go to the working folder:
bash
Copy
Edit
cd /root/nvidia
2. Confirm Python 3.9 is working:
bash
Copy
Edit
./python39-offline/usr/local/bin/python3.9 --version
You should see:

nginx
Copy
Edit
Python 3.9.18
3. Create a virtual environment named torch-env
bash
Copy
Edit
./python39-offline/usr/local/bin/python3.9 -m venv /root/torch-env
This creates the venv in /root/torch-env/.

4. Activate the virtual environment:
bash
Copy
Edit
source /root/torch-env/bin/activate
Once activated, your prompt should change like:

graphql
Copy
Edit
(torch-env) root@rhel86-barem #
5. Install PyTorch offline using the .whl files
bash
Copy
Edit
pip install --no-index --find-links=/root/nvidia/pytorch-cu121-offline torch torchvision torchaudio
This tells pip to:

Not use the internet (--no-index)

Install from local .whl files

âœ… Wait for it to finish (it may take a minute).

6. Test if PyTorch + GPU is working
bash
Copy
Edit
python -c "import torch; print(torch.__version__); print(torch.cuda.is_available()); print(torch.cuda.get_device_name(0))"
Expected Output:

graphql
Copy
Edit
2.x.x
True
NVIDIA H100 SXM
âœ… Summary of Commands to Copy-Paste:
bash
Copy
Edit
cd /root/nvidia
./python39-offline/usr/local/bin/python3.9 -m venv /root/torch-env
source /root/torch-env/bin/activate
pip install --no-index --find-links=/root/nvidia/pytorch-cu121-offline torch torchvision torchaudio
python -c "import torch; print(torch.__version__); print(torch.cuda.is_available()); print(torch.cuda.get_device_name(0))"
Let me know if you want a GPU test script (like matrix multiplication or speed test).









Ask ChatGPT
























âœ… PART 1: Install Python 3.9 Locally and Package for Transfer
1. On Local Ubuntu (with internet)
bash
Copy
Edit
sudo apt update
sudo apt install -y build-essential libssl-dev zlib1g-dev \
  libbz2-dev libreadline-dev libsqlite3-dev wget curl llvm \
  libncursesw5-dev xz-utils tk-dev libxml2-dev libxmlsec1-dev libffi-dev liblzma-dev
2. Download and build Python 3.9
bash
Copy
Edit
cd /tmp
wget https://www.python.org/ftp/python/3.9.18/Python-3.9.18.tgz
tar -xf Python-3.9.18.tgz
cd Python-3.9.18
./configure --enable-optimizations
make -j$(nproc)
make install DESTDIR=$HOME/python39-offline
This installs Python into $HOME/python39-offline/usr/local/bin/python3.9

3. Compress the folder
bash
Copy
Edit
cd ~
tar -czvf python39-offline.tar.gz python39-offline
âœ… PART 2: Copy Python 3.9 + PyTorch Offline to VM
bash
Copy
Edit
scp -i veera-dev2.pem ~/python39-offline.tar.gz ec2-user@<VM-IP>:/home/ec2-user/
scp -i veera-dev2.pem pytorch-cu121-offline.tar.gz ec2-user@<VM-IP>:/home/ec2-user/
âœ… PART 3: On VM â€“ Extract Python and Use It
1. SSH into the VM
bash
Copy
Edit
ssh -i veera-dev2.pem ec2-user@<VM-IP>
2. Extract Python
bash
Copy
Edit
tar -xzvf python39-offline.tar.gz
cd python39-offline
3. Add Python 3.9 to PATH (temporary)
bash
Copy
Edit
export PATH=$PWD/usr/local/bin:$PATH
4. Check Python version
bash
Copy
Edit
python3.9 --version
âœ… PART 4: Install PyTorch Offline with Python 3.9
1. Extract your PyTorch package
bash
Copy
Edit
tar -xzvf pytorch-cu121-offline.tar.gz
This should create a folder like pytorch-cu121-offline/ containing .whl and .tar.gz files.

2. Create and activate virtualenv
bash
Copy
Edit
python3.9 -m venv torch-env
source torch-env/bin/activate
3. Install from local folder
bash
Copy
Edit
pip install --no-index --find-links=./pytorch-cu121-offline torch torchvision torchaudio
âœ… PART 5: Verify GPU Support
bash
Copy
Edit
python -c "import torch; print(torch.cuda.is_available()); print(torch.cuda.get_device_name(0))"
ðŸ§¼ Optional (Make Python 3.9 Permanent)
If you want Python 3.9 globally:

bash
Copy
Edit
sudo cp -r python39-offline/usr/local/* /usr/local/
sudo ln -sf /usr/local/bin/python3.9 /usr/bin/python3.9
Let me know if you also want a TensorFlow offline zip to be used with this Python 3.9, or if you want .whl structure for PyTorch.









Ask ChatGPT
